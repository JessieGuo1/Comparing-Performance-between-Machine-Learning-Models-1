# Comparing Performance between Machine Learning Models 1
Comparing performance between machine learning models linear regression, logistic regression, and neural networks for a dataset of ~4000 observations, where draft2.py is the latest code.

In this project, we compare the effectiveness of using a neural network to accurately predict future/unseen responses (outputs) with using linear and logistic regressions. Here, we are using neural networks for both regression and classification, where we adjust the response variable for the Abalone dataset (https://archive.ics.uci.edu/dataset/1/abalone) from the UC Irvine Machine Learning Repository to suit a classification problem (ages ≥ 7 years vs. < 7 years). The metrics for comparison are root mean squared error (RMSE) and R2 correlation coefficient for regression (linear regression, neural network for regression), and accuracy, Area Under The Curve (AUC) and Receiver Operating Characteristic (ROC) curve for classification (logistic regression, neural network for classification). We also compare the metrics between normalised features and features not normalised, and between models with all features used in training and models with only two of the most correlated features.

We build linear models (ordinary least squares and logistic regression) and multilayer perceptron neural networks (MLP models) with stochastic gradient descent. Under 60/40 train-test splits and across 20 experiments, linear regression reached a mean RMSE ≈ 2.2 and R2 ≈ 0.53 while the classification task using logistic regression gave an accuracy ≈ 0.93 and an AUC score of ≈ 0.95. The MLP models produced similar results, but did not consistently surpass our linear models, indicating it is perhaps better to use linear models for their interpretability. As it seems that the data is already scaled within [0, 1], further normalisation did not have any significant effect on the results.

This project is an introductory glimpse into machine learning, exploring linear models and multilayer perceptrons on the Abalone dataset for regression and binary classification. Using a variety of data-preprocessing techniques and model evaluation, we have found that linear models are a very effective tool to explain the difference in ring counts amongst abalones. We have also investigated MLP Neural Networks with ReLU, offering a clear improvement to linear models when restricted to two features. All models performed very well on the classification problem, and had very high AUC scores, perhaps indicating that the data is close to being linearly separable. There were also other interesting observations, such as the minimal effect of normalising the data, and that using all features consistently outperforms two-feature models for this dataset. Future improvements include adaptive optimisers, regularisation and better hyperparameter fine-tuning.
